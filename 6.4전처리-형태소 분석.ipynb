{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제목, 부제목, 본문별 형태소 분석. 연도별 컬럼.\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "from bareunpy import Tagger\n",
    "\n",
    "# 설치한 자신의 호스트에 접속합니다.\n",
    "# 아래에 \"https://bareun.ai/\"에서 이메일 인증 후 발급받은 API KEY(\"koba-...\"\")를 입력해주세요. \"로그인-내정보 확인\"\n",
    "my_tagger = Tagger('API KEY', 'localhost', port=5656) # <- 본인의 API KEY로 교체 \n",
    "\n",
    "# 사용자 사전 저장\n",
    "# 사전 이름으로 영문, 숫자, 기호('-', '_')만 사용 가능\n",
    "# 사전 단어에 기호는 추가될 수 없습니다(복합명사 분리사전의 ^ 기호 제외).\n",
    "cust_dic = my_tagger.custom_dict(\"pkn_dict_01\") \n",
    "cust_dic.copy_np_set({\n",
    "    \"거리의_악사\", \"겨울비\", \"계산\", \"군식구\", \"귀족\", \"그_형제의_연인들\", \"기다리는_불안\", \"까치설\", \"꿈꾸는_자가_창조한다\", \"나비야_청산가자\",\n",
    "    \"나비와_엉겅퀴\", \"내_마음은_호수\", \"노을_진_들녘\", \"녹지대\", \"눈먼_식솔\", \"단층\", \"도선장\", \"도시의_고양이들\", \"도표없는_길\", \"돌아온_고양이\",\n",
    "    \"돌아온_아이\", \"만리장성의_나라\", \"만화_토지\", \"목련_밑\", \"못_떠나는_배\", \"밀고자\", \"반딧불\", \"뱁새족\", \"버리고_갈_것만_남아서_참_홀가분하다\", \"벽지\",\n",
    "    \"불신시대\", \"비는_내린다\", \"새벽의_합창\", \"생명의_아픔\", \"속단\", \"시경소화\", \"시장과_전장\", \"신교수의_부인\", \"쌍두아\", \"암흑시대\",\n",
    "    \"암흑의_사자\", \"애가\", \"어느_오후의_결정\", \"어머니\", \"영주와_고양이\", \"옛날_이야기\", \"옛날의_그집\", \"외곽지대\", \"우리들의_시간\", \"우화\",\n",
    "    \"원주통신\", \"은하\", \"은하수\", \"인간\", \"일본산고\", \"재귀열\", \"재혼의_조건\", \"전도\", \"죄인들의_숙제\", \"집\",\n",
    "    \"창\", \"창작실기론\", \"청소년_토지\", \"타인들\", \"토지\", \"파시\", \"평면도\", \"표류도\", \"푸른_운하\", \"풍경A\",\n",
    "    \"풍경B\", \"하루\", \"해동여관의_미나\", \"호수\", \"환상의_시기\", \"훈향\", \"흑백_콤비의_구두\", \"흑흑백백\", \"희오의_바다\", \"사랑섬_할머니\", \"Q씨에게\", \"가을에_온_여인\", \"김약국의_딸들\", \"문학을_사랑하는_젊은이들에게\", \"성녀와_마녀\", \"시장과_전장\", \"신교수의_부인\", \"애가\", \"어느_오후의_결정\", \"영원한_반려\", \"설화\", \"노을진_들녘\", \"노을진들녘\"\n",
    "\n",
    "    \"강만길\", \"김동리\", \"김영주\", \"김지하\", \"김치수\", \"김형국\", \"유종호\", \"이어령\", \"이종환\", \"박경리\",\n",
    "    \"박금이\", \"박완서\", \"박화성\", \"백낙청\", \"백철\",   \"전광용\", \"정인영\", \"정현기\", \"최일남\",\n",
    "\n",
    "    \"가정생활\", \"가톨릭신문\", \"경향신문\", \"국제신보\", \"동아일보\", \"문학과지성\", \"문학사상\", \"문화일보\", \"민주신보\", \"부산일보\",\n",
    "    \"사상계\", \"새벗\", \"세계의_문학\", \"세대\", \"소설문학\", \"숙란\", \"신동아\", \"신작15인집\", \"신태양\", \"여상\",\n",
    "    \"여성동아\", \"여원\", \"월간경항\", \"월간문학\", \"월간중앙\", \"자유공론\", \"잡지\", \"전남일보\", \"전망\", \"정경문화\",\n",
    "    \"조선일보\", \"주부생활\", \"중앙일보\", \"한국문학\", \"한국일보\", \"한국평론\", \"현대문학\",\n",
    "\n",
    "    \"금관문화훈장\", \"내성문학상\", \"대한교과서\", \"동광출판사\", \"동민문화사\",\n",
    "    \"만주\", \"민음사\", \"박경리_기념관\", \"박경리문학공원\", \"박경리문학관\",\n",
    "    \"박경리문학상\", \"박경리문학제\", \"범우사\", \"보관문화훈장\", \"삼성출판사\",\n",
    "    \"서문당\", \"서울대학교\", \"소설토지학교\", \"솔출판사\", \"수문서관\",\n",
    "    \"신태양사\", \"연세대학교\", \"원주\", \"월탄문학상\", \"을유문화사\",\n",
    "    \"이화여자대학교\", \"인촌상\", \"일본제국\", \"일월서각\", \"정릉동\",\n",
    "    \"지리산\", \"지식산업사\", \"진주여자고등학교\", \"충무\", \"토지문학제\",\n",
    "    \"토지문화관\", \"토지문화재단\", \"토지사랑회\", \"통영\", \"평사리\",\n",
    "    \"하동\", \"한국여류문학상\", \"한국여류문학인회\", \"현대문학사\", \"현대문학상\",\n",
    "    \"현암사\", \"호암예술상\", \"환경운동연합\",\n",
    "\n",
    "    '박경리씨',\n",
    "\n",
    "})\n",
    "cust_dic.update()\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "from bareunpy import Tagger\n",
    "import os\n",
    "import string\n",
    "\n",
    "# API 키 설정 및 태거 설정\n",
    "API_KEY = \"API KEY\"  # 본인의 API KEY로 교체\n",
    "my_tagger = Tagger(API_KEY, 'localhost', port=5656)\n",
    "\n",
    "# 사용자 정의 사전 로드 및 설정\n",
    "cust_dict2 = my_tagger.custom_dict(\"pkn_dict_01\")\n",
    "cust_dict2.load()\n",
    "\n",
    "# 불용어 리스트 로드 함수 (모든 특수문자 포함)\n",
    "def load_stopwords():\n",
    "    punctuation = set(string.punctuation)\n",
    "    extra_symbols = {'＇', '\\\"', '(', ')', '[', ']', '{', '}', '<', '>', '《', '》', '｢', '｣', '󰡔', '󰡕', '【', '】'}\n",
    "    return {\"박경리\", \"우리\", *punctuation, *extra_symbols}\n",
    "\n",
    "# 불용어 로딩\n",
    "stopwords = load_stopwords()\n",
    "\n",
    "# NNP 태그에 해당하는 명사만 필터링 및 불용어 제거 함수\n",
    "def get_filtered_nouns(text):\n",
    "    my_tagger.set_domain('pkn_dict_01')  # 사용자 사전 적용\n",
    "    \n",
    "    # 형태소 분석 수행\n",
    "    pos_result = my_tagger.pos(text)\n",
    "    \n",
    "    # NNP 태그로 필터링하여 불용어 제거\n",
    "    nnp_nouns = [word for word, tag in pos_result if tag == 'NNP' and word not in stopwords and len(word) >= 2]\n",
    "\n",
    "    return nnp_nouns\n",
    "\n",
    "# 엑셀 파일 분석 및 모든 컬럼에서 NNP 명사를 추출하여 연도별 시트로 저장하는 함수\n",
    "def analyze_combined_nnp_by_year(folder_path, file_name, date_column, content_columns):\n",
    "    # 파일 경로 및 파일 존재 확인\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"File not found: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Reading file: {file_path}\")\n",
    "    df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "    # 날짜를 문자열로 변환하여 처리\n",
    "    df[date_column] = df[date_column].astype(str)\n",
    "    print(f\"Converted dates to strings. Sample dates:\\n{df[date_column].head()}\")\n",
    "\n",
    "    # 연도 구간 설정\n",
    "    decade_ranges = [\n",
    "        (\"19550101\", \"19641231\"),\n",
    "        (\"19650101\", \"19741231\"),\n",
    "        (\"19750101\", \"19841231\"),\n",
    "        (\"19850101\", \"19941231\"),\n",
    "        (\"19950101\", \"20041231\"),\n",
    "        (\"20050101\", \"20141231\"),\n",
    "        (\"20150101\", \"20241231\")\n",
    "    ]\n",
    "\n",
    "    # 모든 연도 구간의 NNP 데이터 저장을 위한 딕셔너리\n",
    "    combined_nnp_data = {}\n",
    "\n",
    "    for start_date, end_date in decade_ranges:\n",
    "        # 해당 연도 구간의 데이터 필터링\n",
    "        decade_df = df[(df[date_column] >= start_date) & (df[date_column] <= end_date)]\n",
    "        \n",
    "        date_range = f\"{start_date[:4]}-{end_date[:4]}\"\n",
    "        print(f\"\\nAnalyzing decade: {date_range}, Rows: {len(decade_df)}\")\n",
    "\n",
    "        if decade_df.empty:\n",
    "            print(f\"No data found for {date_range}. Skipping...\")\n",
    "            continue\n",
    "\n",
    "        nnp_nouns = []\n",
    "        for column_name in content_columns:\n",
    "            for text in decade_df[column_name].dropna():\n",
    "                nnp_nouns.extend(get_filtered_nouns(text))\n",
    "\n",
    "        combined_nnp_data[date_range] = nnp_nouns\n",
    "\n",
    "    # 최대 리스트 길이에 맞춰서 빈 칸 추가\n",
    "    max_len = max(len(v) for v in combined_nnp_data.values())\n",
    "    for key in combined_nnp_data:\n",
    "        combined_nnp_data[key].extend([\"\"] * (max_len - len(combined_nnp_data[key])))\n",
    "\n",
    "    # 결과를 데이터프레임으로 생성하고 파일 저장\n",
    "    output_file = os.path.join(folder_path, f\"{os.path.splitext(file_name)[0]}_combined_nnp_filtered.xlsx\")\n",
    "    combined_df = pd.DataFrame(combined_nnp_data)\n",
    "    combined_df.to_excel(output_file, sheet_name=\"Combined_NNP\", index=False)\n",
    "    print(f\"Combined NNP data saved to '{output_file}'.\")\n",
    "\n",
    "# 사용 예시\n",
    "folder_path = \"경로\"\n",
    "file_name = \"파일명.xlsx\"\n",
    "date_column = \"date\"  # 날짜 정보가 있는 컬럼 이름\n",
    "content_columns = [\"title\", \"sub-title\", \"content\"]  # 분석할 컬럼 이름들\n",
    "analyze_combined_nnp_by_year(folder_path, file_name, date_column, content_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 워드 클라우드(미완)\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# 한글 글꼴 경로 설정 (예: Malgun Gothic)\n",
    "font_path = \"C:/Windows/Fonts/malgun.ttf\"  # Windows의 맑은 고딕 폰트 경로\n",
    "\n",
    "# TF-IDF 분석 준비 함수\n",
    "def load_and_prepare_data(folder_path, file_name):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    nnp_df = pd.read_excel(file_path, sheet_name=\"NNP\").fillna(\"\")\n",
    "    nng_df = pd.read_excel(file_path, sheet_name=\"NNG\").fillna(\"\")\n",
    "\n",
    "    # 연도별 데이터를 결합하여 하나의 텍스트로 만듦\n",
    "    nnp_texts = [\" \".join(row.dropna()) for _, row in nnp_df.iterrows()]\n",
    "    nng_texts = [\" \".join(row.dropna()) for _, row in nng_df.iterrows()]\n",
    "\n",
    "    all_texts = nnp_texts + nng_texts\n",
    "    return all_texts\n",
    "\n",
    "# TF-IDF 분석 수행 함수\n",
    "def perform_tfidf_analysis(all_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "# 결과를 엑셀에 저장\n",
    "def save_tfidf_results(tfidf_df, folder_path, file_name, date_ranges):\n",
    "    output_file = os.path.join(folder_path, f\"{os.path.splitext(file_name)[0]}_tfidf_results.xlsx\")\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        for i, date_range in enumerate(date_ranges):\n",
    "            tfidf_df.iloc[[i]].transpose().rename(columns={i: date_range}).to_excel(writer, sheet_name=date_range)\n",
    "    print(f\"TF-IDF 분석 결과가 '{output_file}'에 저장되었습니다.\")\n",
    "\n",
    "# 워드 클라우드 생성 함수\n",
    "def generate_wordcloud(text, title, save_path, font_path):\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white', font_path=font_path).generate(text)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "    print(f\"워드 클라우드가 '{save_path}'에 저장되었습니다.\")\n",
    "\n",
    "# 연도별 워드 클라우드 생성 및 저장\n",
    "def create_wordclouds(tfidf_df, date_ranges, folder_path, content_type, font_path):\n",
    "    for i, date_range in enumerate(date_ranges):\n",
    "        words = tfidf_df.iloc[i].to_dict()\n",
    "        sorted_words = {k: v for k, v in sorted(words.items(), key=lambda item: item[1], reverse=True)}\n",
    "        text = \" \".join([k for k, v in sorted_words.items() if v > 0])\n",
    "\n",
    "        save_path = os.path.join(folder_path, f\"{content_type}_{date_range}_wordcloud.png\")\n",
    "        generate_wordcloud(text, f\"{content_type} {date_range}\", save_path, font_path)\n",
    "\n",
    "# 사용 예시\n",
    "folder_path = r\"경로\"  # 파일이 위치한 폴더 경로\n",
    "file_name = \"파일명.xlsx\"  # TF-IDF 분석할 파일 이름\n",
    "date_ranges = [\"19550101-19641231\", \"19650101-19741231\", \"19750101-19841231\", \"19850101-19941231\",\n",
    "               \"19950101-20041231\", \"20050101-20141231\", \"20150101-20241231\"]\n",
    "\n",
    "all_texts = load_and_prepare_data(folder_path, file_name)\n",
    "tfidf_df = perform_tfidf_analysis(all_texts)\n",
    "save_tfidf_results(tfidf_df, folder_path, file_name, date_ranges)\n",
    "\n",
    "# 워드 클라우드 생성 (타이틀, 서브타이틀, 컨텐츠 구분하여 생성)\n",
    "content_types = [\"title\", \"sub-title\", \"content\"]\n",
    "for content_type in content_types:\n",
    "    create_wordclouds(tfidf_df, date_ranges, folder_path, content_type, font_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시각화\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# TF-IDF 분석 준비 함수\n",
    "def load_and_prepare_data(folder_path, file_name):\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    nnp_df = pd.read_excel(file_path, sheet_name=\"Combined_NNP\").fillna(\"\")\n",
    "    # nng_df = pd.read_excel(file_path, sheet_name=\"NNG\").fillna(\"\")\n",
    "\n",
    "    # 연도별 데이터를 결합하여 하나의 텍스트로 만듦\n",
    "    nnp_texts = [\" \".join(row.dropna()) for _, row in nnp_df.iterrows()]\n",
    "    # nng_texts = [\" \".join(row.dropna()) for _, row in nng_df.iterrows()]\n",
    "\n",
    "    all_texts = nnp_texts \n",
    "    # + nng_texts\n",
    "    return all_texts\n",
    "\n",
    "# TF-IDF 분석 수행 함수\n",
    "def perform_tfidf_analysis(all_texts):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(all_texts)\n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    return tfidf_df\n",
    "\n",
    "# 연도별 상위 50개 키워드 추출 함수\n",
    "def get_top_keywords(tfidf_df, date_ranges, top_n=50):\n",
    "    top_keywords_by_year = {}\n",
    "    for i, date_range in enumerate(date_ranges):\n",
    "        top_keywords = tfidf_df.iloc[i].sort_values(ascending=False).head(top_n)\n",
    "        top_keywords_by_year[date_range] = top_keywords\n",
    "    return top_keywords_by_year\n",
    "\n",
    "# 한글 폰트 설정\n",
    "plt.rcParams['font.family'] = 'Malgun Gothic'  # Windows에서의 맑은 고딕\n",
    "plt.rcParams['axes.unicode_minus'] = False  # 마이너스 부호가 깨지는 현상 방지\n",
    "\n",
    "# 키워드 변화 추이 시각화 함수\n",
    "def plot_keyword_trends(top_keywords_by_year, content_type=\"Title\"):\n",
    "    trend_data = pd.DataFrame(top_keywords_by_year).fillna(0)\n",
    "\n",
    "    plt.figure(figsize=(14, 10))\n",
    "    ax = sns.heatmap(trend_data, cmap=\"YlGnBu\", annot=False, cbar=True, linewidths=0.5, linecolor='gray')\n",
    "    \n",
    "    # 각 연도 구간에서 가장 높은 TF-IDF 값을 가진 키워드에 별도 표시\n",
    "    for col, date_range in enumerate(trend_data.columns):\n",
    "        max_keyword = trend_data[date_range].idxmax()  # 가장 높은 TF-IDF 값을 가진 키워드\n",
    "        max_value = trend_data[date_range].max()\n",
    "        \n",
    "        # 가장 높은 값을 가진 셀에 텍스트로 값을 표시\n",
    "        ax.text(col + 0.5, trend_data.index.get_loc(max_keyword) + 0.5,\n",
    "                f\"{max_keyword} ({max_value:.2f})\",\n",
    "                color=\"red\", ha=\"center\", va=\"center\", fontsize=15, fontweight=\"bold\")\n",
    "    \n",
    "    plt.xlabel(\"Year\")\n",
    "    plt.ylabel(\"Keyword\")\n",
    "    plt.title(f\"Top 50 Keyword Trends Over Time - {content_type}\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 사용 예시\n",
    "folder_path = r\"경로\"\n",
    "file_name = \"파일명.xlsx\"\n",
    "date_ranges = [\"19550101-19641231\", \"19650101-19741231\", \"19750101-19841231\", \"19850101-19941231\",\n",
    "               \"19950101-20041231\", \"20050101-20141231\", \"20150101-20241231\"]\n",
    "\n",
    "# 데이터 준비 및 TF-IDF 수행\n",
    "all_texts = load_and_prepare_data(folder_path, file_name)\n",
    "tfidf_df = perform_tfidf_analysis(all_texts)\n",
    "\n",
    "# 연도별 상위 50개 키워드 추출\n",
    "top_keywords_by_year = get_top_keywords(tfidf_df, date_ranges)\n",
    "\n",
    "# 키워드 변화 추이 시각화\n",
    "plot_keyword_trends(top_keywords_by_year, content_type=\"Content\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
