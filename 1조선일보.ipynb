{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 조선 뉴스 라이브러리100(~19991231) 목록, 본문\n",
    "\n",
    "import os\n",
    "import time\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "chrome_options = Options()\n",
    "# chrome_options.add_argument(\"--headless\")  # 브라우저 창을 띄우지 않음\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 비활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# 크롬 브라우저 실행 (최신 Selenium은 ChromeDriver 설치 필요 없음)\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "\n",
    "def login_kakao(username, password):\n",
    "    \"\"\"카카오 로그인 페이지로 직접 접근하여 로그인합니다.\"\"\"\n",
    "    try:\n",
    "        login_url = \"https://www.chosun.com/subscribe/signin/\"\n",
    "        driver.get(login_url)\n",
    "\n",
    "        # 아이디 입력 필드가 나타날 때까지 대기하고 입력\n",
    "        username_field = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"username\"))\n",
    "        )\n",
    "        username_field.send_keys(username)\n",
    "\n",
    "        # 비밀번호 입력 필드가 나타날 때까지 대기하고 입력\n",
    "        password_field = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.ID, \"subsPassword\"))\n",
    "        )\n",
    "        password_field.send_keys(password)\n",
    "\n",
    "        # 로그인 버튼 클릭\n",
    "        login_button = WebDriverWait(driver, 10).until(\n",
    "            EC.element_to_be_clickable((By.ID, \"subsSignIn\"))\n",
    "        )\n",
    "        login_button.click()\n",
    "        time.sleep(3)\n",
    "        print(\"로그인 버튼 클릭 성공!\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"로그인 중 오류 발생: {e}\")\n",
    "\n",
    "\n",
    "def fetch_articles(search_query, start_date, end_date):\n",
    "    \"\"\"특정 검색어에 대해 기사를 검색하고 정보를 수집.\"\"\"\n",
    "    article_list = []\n",
    "    page_number = 0\n",
    "    sort_order = 1\n",
    "    base_url = \"https://newslibrary.chosun.com/search/search_result.html\"\n",
    "\n",
    "    while True:\n",
    "        # 검색 URL 생성\n",
    "        search_url = (\n",
    "            f\"{base_url}?case_num=1&sort={sort_order}&page={page_number}&size=10\"\n",
    "            f\"&query={search_query}&date=date_select&ds={start_date}&de={end_date}&field=&type=&wrt=&set_date=\"\n",
    "        )\n",
    "\n",
    "        # 검색 페이지 열기\n",
    "        driver.get(search_url)\n",
    "\n",
    "        # 기사 목록이 로드될 때까지 대기\n",
    "        try:\n",
    "            WebDriverWait(driver, 5).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"search_news\"))\n",
    "            )\n",
    "        except:\n",
    "            print(f\"{search_query} - 더 이상 페이지가 없습니다.\")\n",
    "            break\n",
    "\n",
    "        print(f\"{search_query} - {page_number} 페이지를 처리 중입니다...\")\n",
    "        \n",
    "        # HTML 파싱\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "        # 기사 목록 추출\n",
    "        articles = soup.find_all(\"dl\", class_=\"search_news\")\n",
    "        if not articles:\n",
    "            break  # 더 이상 기사가 없으면 종료\n",
    "\n",
    "        # 기사 정보 추출\n",
    "        for article in articles:\n",
    "            title_tag = article.find(\"dt\").find(\"a\")\n",
    "            if title_tag:\n",
    "                title = title_tag.text.strip()\n",
    "                url = \"https://newslibrary.chosun.com\" + title_tag[\"href\"]\n",
    "\n",
    "                # 날짜, 면수, 섹션, 저자 정보 추출\n",
    "                art_info_tag = article.find(\"dd\", class_=\"art_info\")\n",
    "                date = art_info_tag.contents[0].strip() if art_info_tag.contents else \"\"\n",
    "                spans = art_info_tag.find_all(\"span\")\n",
    "                page = spans[0].text.strip() if len(spans) > 0 else \"\"\n",
    "                section = spans[1].text.strip() if len(spans) > 1 else \"\"\n",
    "                writer = spans[2].text.strip() if len(spans) > 2 else \"\"\n",
    "\n",
    "                # 본문 수집\n",
    "                content = clean_content(url)\n",
    "\n",
    "                # 기사 정보 리스트에 추가\n",
    "                article_list.append({\n",
    "                    \"media\": '조선일보',\n",
    "                    \"title\": title,\n",
    "                    \"date\": date,\n",
    "                    \"page\": page,\n",
    "                    \"section\": section,\n",
    "                    'article_type': '',\n",
    "                    \"writerr\": writer,\n",
    "                    \"url\": url,\n",
    "                    \"content\": content,\n",
    "                })\n",
    "\n",
    "        # 다음 페이지로 이동\n",
    "        page_number += 1\n",
    "\n",
    "    return article_list\n",
    "\n",
    "def clean_content(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        WebDriverWait(driver, 5).until(\n",
    "            EC.presence_of_element_located((By.ID, \"subtitle1\"))\n",
    "        )\n",
    "        time.sleep(1)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        tag = soup.find(\"p\", id=\"subtitle1\")\n",
    "\n",
    "        if tag:\n",
    "            # 불필요한 태그 제거 후 텍스트 추출\n",
    "            for t in tag.find_all([\"span\", \"img\", \"a\"]):\n",
    "                t.decompose()\n",
    "            return tag.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"본문 추출 실패: {e}\")\n",
    "\n",
    "    return \"\"  # 본문이 없을 경우 빈 문자열 반환    \n",
    "\n",
    "def save_to_excel(data, filename):\n",
    "    \"\"\"수집한 데이터를 엑셀 파일로 저장.\"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    df.drop_duplicates(subset=['url'], keep='first', inplace=True)\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# 아래에서 사용자 입력 설정\n",
    "if __name__ == \"__main__\":\n",
    "    # 사용자 정보 및 검색어 설정\n",
    "    username = \"shincha23@naver.com\"\n",
    "    password = \"sje126604!\"\n",
    "    search_queries = [\"검색어색어\"]\n",
    "    start_date = \"19610101\"\n",
    "    end_date = \"19611231\"\n",
    "\n",
    "    # 조선일보 로그인\n",
    "    login_kakao(username, password)\n",
    "\n",
    "    # 모든 검색어에 대해 기사 수집\n",
    "    all_articles = []\n",
    "    for query in search_queries:\n",
    "        articles = fetch_articles(query, start_date, end_date)\n",
    "        all_articles.extend(articles)    \n",
    "\n",
    "    # 결과를 엑셀 파일로 저장\n",
    "    save_to_excel(all_articles, f\"조선일보 라이브러리 {start_date}~{end_date}.xlsx\")\n",
    "\n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "\n",
    "    print(\"스크래핑이 완료되었습니다!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 뉴스탭 기사 목록(19930101부터 가능)\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import urllib.parse\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 필요시 활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 키워드와 날짜 범위 설정\n",
    "keywords = [\n",
    "    \"검색어\",\n",
    "]\n",
    "start_date = \"19930101\"\n",
    "end_date = \"20240851\"\n",
    "\n",
    "\n",
    "seen_urls = set()  # 중복된 URL을 저장하기 위한 집합\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def fetch_search_results(keyword, start_date, end_date):\n",
    "    \"\"\"키워드와 날짜 범위에 따라 기사를 스크래핑합니다.\"\"\"\n",
    "    base_url = \"https://www.chosun.com/nsearch/?query=\"\n",
    "    page = 1  # 페이지 시작 번호 설정\n",
    "    results = []\n",
    "    previous_articles = set()  # 이전 페이지의 기사 URL 또는 제목을 저장할 집합\n",
    "\n",
    "    while True:\n",
    "        print(f\"{keyword} 페이지 {page}\")\n",
    "        search_url = (\n",
    "            f\"{base_url}{urllib.parse.quote(keyword)}&\"\n",
    "            f\"page={page}&siteid=www&sort=1&date_period=&date_start={start_date}&date_end={end_date}&\"\n",
    "            \"writer=&field=&emd_word=&expt_word=&opt_chk=false&app_check=0&website=www,chosun&category=\"\n",
    "        )\n",
    "        driver.get(search_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        try:\n",
    "            # 기사 목록 가져오기\n",
    "            search_feed_element = WebDriverWait(driver, 10).until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"search-feed\"))\n",
    "            )\n",
    "            search_feed_html = search_feed_element.get_attribute(\"outerHTML\")\n",
    "            search_feed = BeautifulSoup(search_feed_html, 'html.parser')\n",
    "            articles = search_feed.find_all(\"div\", class_=\"story-card-wrapper\") if search_feed else []\n",
    "\n",
    "            print(f\"찾은 기사 수: {len(articles)}\")  # 디버깅용 출력\n",
    "\n",
    "            if not articles:\n",
    "                print(f\"{keyword} - 더 이상 기사가 없습니다.\")\n",
    "                break  # 더 이상 기사가 없으면 루프 종료\n",
    "\n",
    "            current_articles = set()  # 현재 페이지의 기사 URL 또는 제목을 저장할 집합\n",
    "\n",
    "            # 기사 정보 추출\n",
    "            for article in articles:\n",
    "                try:\n",
    "                    title_tag = article.find(\"a\", class_=\"text__link story-card__headline | box--margin-none text--black font--primary h3 text--left\")\n",
    "                    title = title_tag.text.strip() if title_tag else \"\"\n",
    "                    url = title_tag[\"href\"] if title_tag else \"\"         \n",
    "\n",
    "                    # 기사 식별자를 URL 또는 제목으로 설정\n",
    "                    article_identifier = url  # 또는 title\n",
    "\n",
    "                    current_articles.add(article_identifier)\n",
    "\n",
    "                    span_elements = article.find_all(\"span\")\n",
    "                    date = span_elements[-1].text.strip() if span_elements else \"\"\n",
    "\n",
    "                    section_element = article.find(\"span\", class_=\"hover-underline | box--pointer\")\n",
    "                    section = section_element.text.strip() if section_element else \"\"\n",
    "\n",
    "                    writer_element = article.find(\"span\", class_=\"box--hidden-sm\")\n",
    "                    writer = writer_element.text.strip().replace('|', '').strip() if writer_element else \"\"\n",
    "\n",
    "                    # 결과 리스트에 추가\n",
    "                    results.append({\n",
    "                        'media': '조선일보',\n",
    "                        'title': title,\n",
    "                        'date': date,\n",
    "                        'section': section,\n",
    "                        'article_type': '',\n",
    "                        'writer': writer,\n",
    "                        'page': '',\n",
    "                        'url': url,\n",
    "                        'content': ''  # 컨텐츠 정보 없음\n",
    "                    })\n",
    "                except Exception as e:\n",
    "                        print(f\"기사 추가됨: {title}, {url}\")  # 디버그 메시지\n",
    "\n",
    "            # 이전 페이지의 기사와 현재 페이지의 기사를 비교하여 동일하면 루프 종료\n",
    "            if previous_articles == current_articles and len(current_articles) == 10:\n",
    "                print(f\"{keyword} - 이전 페이지와 동일한 10개의 기사가 발견되어 루프를 종료합니다.\")\n",
    "                break\n",
    "            else:\n",
    "                previous_articles = current_articles  # 이전 페이지의 기사를 현재 페이지의 기사로 업데이트\n",
    "                page += 1  # 다음 페이지로 이동\n",
    "\n",
    "            # 만약 현재 페이지의 기사가 10개 미만이면 루프 종료 (필요 시 주석 해제)\n",
    "            if len(current_articles) < 10:\n",
    "                print(f\"{keyword} - 마지막 페이지에 도달했습니다.\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"페이지 로딩 오류 발생: {e}\")\n",
    "            break  # 오류 발생 시 루프 종료\n",
    "\n",
    "    return results  # 모든 페이지를 탐색한 후 최종 결과 반환\n",
    "\n",
    "\n",
    "# 엑셀 저장 함수\n",
    "def save_to_excel(data, filename):\n",
    "    df = pd.DataFrame(data)\n",
    "    df = df[~df['title'].str.contains('블락비|지코|학폭|뇌섹남|나인뮤지스', case=False, na=False)]\n",
    "    df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "\n",
    "# 모든 키워드에 대해 검색 수행\n",
    "all_results = []\n",
    "for keyword in keywords:\n",
    "    # 매개변수 전달\n",
    "    results = fetch_search_results(keyword, start_date, end_date)\n",
    "    all_results.extend(results)\n",
    "\n",
    "# 결과를 엑셀로 저장\n",
    "save_to_excel(all_results, f'1조선일보 {start_date}~{end_date}.xlsx')\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#본문 기사 저장하기\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 출처와 파일명 변수 설정\n",
    "file_name = '파일명'  # 파일명 설정\n",
    "base_path = rf'경로'  # 경로 설정\n",
    "\n",
    "# 2. 임시 파일 경로와 최종 파일 경로 생성\n",
    "temp_output_path = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "# 임시 파일 에러를 대비해 본 파일에 덮어쓴다.\n",
    "output_path = os.path.join(base_path, f'{file_name}_본문.xlsx')  # 최종 파일 경로\n",
    "error_log_path = os.path.join(base_path, 'error_log.txt')  # 오류 로그 경로\n",
    "\n",
    "# 3. 파일 경로 생성 및 엑셀 파일 불러오기\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# Selenium 헤드리스 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# Chrome 브라우저 실행\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# URL에서 본문을 스크랩하는 함수 정의\n",
    "def get_content_from_url(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        if \"원하시는 페이지를 찾을 수 없습니다\" in driver.page_source:\n",
    "            print(f\"Page not found: {url}\")\n",
    "            return None, None\n",
    "\n",
    "        # CSS 선택자 리스트\n",
    "        possible_selectors = [\n",
    "            '.article-body', '#article-body', '.par', '#par',\n",
    "            '.read-news.article', '#read-news.article', '.sticky-article', '#sticky-article',\n",
    "            '.articleBody', '#articleBody', '.contents', '#contents',\n",
    "            '.news_body', '#news_body', '.Paragraph', '#Paragraph',\n",
    "            '.article_body', '#article_body', '.content', '#content',\n",
    "            '.article-view-content-div', '#article-view-content-div'\n",
    "        ]\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        all_content = []  # 본문 내용을 저장할 리스트\n",
    "\n",
    "        for selector in possible_selectors:\n",
    "            elements = soup.select(selector)\n",
    "            for element in elements:\n",
    "                for caption in element.find_all(\n",
    "                    ['table', 'div', 'dl', 'figure'],\n",
    "                    class_=['content_popup_ad', 'right_img', 'photo-layout', 'news_imgbox']\n",
    "                ):\n",
    "                    caption.decompose()\n",
    "\n",
    "                for caption in element.find_all('div', style=''):\n",
    "                    caption.decompose()\n",
    "\n",
    "                for caption in element.find_all('figcaption'):\n",
    "                    caption.decompose()                    \n",
    "\n",
    "                ad = element.find('div', class_='ad-area')\n",
    "                if ad:\n",
    "                    next_sibling = ad.find_next_sibling()\n",
    "                    while next_sibling:\n",
    "                        next_sibling.decompose()\n",
    "                        next_sibling = ad.find_next_sibling()\n",
    "                    ad.decompose()\n",
    "\n",
    "                                # 서브타이틀 추출\n",
    "                subtitle = \"\"\n",
    "                subtitle_element = soup.find(\"p\", class_=\"font--primary font--size-md-20 font--size-sm-18 text--black\")\n",
    "                if subtitle_element:\n",
    "                    subtitle = subtitle_element.find(\"span\").get_text(separator=\"\\n\", strip=True) if subtitle_element else \"\"\n",
    "                    subtitle_element.extract()  # 서브 타이틀 제거         \n",
    "\n",
    "                all_content.append(element.get_text(separator='\\n', strip=True))\n",
    "\n",
    "        content = '\\n\\n'.join(all_content) if all_content else \"\"\n",
    "        return content, subtitle\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 스크래핑 진행 상태와 오류 로그 관리\n",
    "error_log = []\n",
    "total_articles = len(df)\n",
    "\n",
    "# 각 행의 URL에서 본문 내용을 스크랩하여 'content' 열에 저장합니다.\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        if pd.notnull(row.get('content')) and row['content'].strip():\n",
    "            print(f\"Skipping {index + 1}/{total_articles}: {row['url']} (content already exists)\")\n",
    "            continue\n",
    "    except AttributeError:\n",
    "        pass\n",
    "\n",
    "    print(f\"Processing {index + 1}/{total_articles}: {row['url']}\")\n",
    "    content, subtitle = get_content_from_url(row['url'])\n",
    "\n",
    "    if content is None:\n",
    "        error_log.append(row['url'])\n",
    "        print(f\"Skipping {row['url']} due to missing content.\")\n",
    "        continue\n",
    "\n",
    "    if content.startswith('='):\n",
    "        content = \"'\" + content\n",
    "\n",
    "    df.at[index, 'content'] = content\n",
    "    df.at[index, 'sub-title'] = subtitle\n",
    "\n",
    "    # 100개마다 임시 파일로 저장\n",
    "    if (index + 1) % 100 == 0:\n",
    "        df.to_excel(temp_output_path, index=False)\n",
    "        print(f\"임시 파일을 저장했습니다: {temp_output_path}\")\n",
    "\n",
    "# 최종 엑셀 파일 저장\n",
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    workbook = writer.book\n",
    "    text_format = workbook.add_format({'num_format': '@'})\n",
    "\n",
    "    df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "    df = df[~df['content'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "    df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    worksheet.set_column(8, 8, None, text_format)\n",
    "\n",
    "# 웹드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 오류 로그 저장\n",
    "if error_log:\n",
    "    with open(error_log_path, 'w') as f:\n",
    "        for url in error_log:\n",
    "            f.write(f\"{url}\\n\")\n",
    "\n",
    "print(\"본문 내용을 성공적으로 스크랩하여 저장했습니다.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
