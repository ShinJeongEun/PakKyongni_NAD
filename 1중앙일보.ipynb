{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기사 목록\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # 이미지 로드 비활성화\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# Keywords to search\n",
    "keywords = [\n",
    "    \"검색어\",\n",
    "]\n",
    "# 검색 기간 설정\n",
    "start_date = \"2024-01-01\"\n",
    "end_date = \"2024-08-31\"    \n",
    "\n",
    "\n",
    "def fetch_articles(search_term, page_number, start_date, end_date):\n",
    "    \"\"\"지정된 검색어와 날짜 범위로 중앙일보에서 기사 스크래핑.\"\"\"\n",
    "    search_url = (\n",
    "        f\"https://www.joongang.co.kr/search/news?\"\n",
    "        f\"keyword={search_term}&startDate={start_date}&endDate={end_date}&\"\n",
    "        f\"sfield=all&page={page_number}\"\n",
    "    )\n",
    "    \n",
    "    driver.get(search_url)\n",
    "\n",
    "    try:\n",
    "        # 명시적인 대기\n",
    "        WebDriverWait(driver, 20).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '.story_list .card'))\n",
    "        )\n",
    "    except:\n",
    "        # 요소가 없으면 빈 리스트 반환\n",
    "        return []\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "    articles = []\n",
    "\n",
    "    for item in soup.select('.story_list .card'):\n",
    "        title = item.select_one('.headline').text.strip() if item.select_one('.headline') else ''\n",
    "        url = item.select_one('a')['href'] if item.select_one('a') else ''\n",
    "        source = item.select_one('.meta .source').text.strip() if item.select_one('.meta .source') else ''\n",
    "        date = item.select_one('.meta .date').text.strip().split(' ')[0] if item.select_one('.meta .date') else ''\n",
    "\n",
    "        articles.append({\n",
    "            \"media\": source,\n",
    "            \"title\": title,\n",
    "            \"date\": date,\n",
    "            \"section\": '',\n",
    "            \"article_type\": '',\n",
    "            'writer': '',\n",
    "            \"page\": '',\n",
    "            \"url\": url,\n",
    "            'content': '',\n",
    "            \"articleId\": ''\n",
    "        })\n",
    "\n",
    "    return articles\n",
    "\n",
    "def save_to_excel(all_articles, filename):\n",
    "    \"\"\"수집한 기사 데이터를 엑셀 파일로 저장.\"\"\"\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "    df.to_excel(filename, index=False)\n",
    "\n",
    "# 각 검색어에 대해 결과 가져오기\n",
    "all_articles = []\n",
    "seen_urls = set()  # 중복 확인을 위한 URL 집합\n",
    "\n",
    "try:\n",
    "    # 웹 드라이버 초기화\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    for term in keywords:\n",
    "        page_number = 1  # 페이지 번호 설정\n",
    "        while True:\n",
    "            print(f\"Fetching articles for '{term}' - Page {page_number}\")\n",
    "            articles = fetch_articles(term, page_number, start_date, end_date)\n",
    "\n",
    "            # 새로운 기사인지 확인하고 중복 제거\n",
    "            new_articles = [article for article in articles if article['url'] not in seen_urls]\n",
    "\n",
    "            # 새로운 기사가 없으면 종료\n",
    "            if not new_articles:\n",
    "                print(f\"No more new articles for '{term}' at page {page_number}. Moving to next keyword.\")\n",
    "                break\n",
    "\n",
    "            # 새로운 기사 URL을 seen_urls에 추가\n",
    "            seen_urls.update(article['url'] for article in new_articles)\n",
    "            all_articles.extend(new_articles)\n",
    "            page_number += 1\n",
    "\n",
    "finally:\n",
    "    # 기사 제목과 링크 저장\n",
    "    filename = f\"1중앙일보 {start_date}~{end_date}.xlsx\"\n",
    "    save_to_excel(all_articles, filename)\n",
    "\n",
    "    # 드라이버 종료\n",
    "    driver.quit()\n",
    "\n",
    "# 결과 확인을 위해 DataFrame 출력 (선택사항)\n",
    "print(pd.DataFrame(all_articles))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 저장sssss\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # 이미지 로드 비활성화\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 1. 출처와 파일명 변수 설정\n",
    "file_name = '파일명'  # 파일명 설정\n",
    "base_path = rf'경로'  # 경로 설정\n",
    "\n",
    "# 2. 임시 파일 경로와 최종 파일 경로 생성\n",
    "temp_output_path = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "# 임시 파일 에러를 대비해 본 파일에 덮어쓴다.\n",
    "output_path = os.path.join(base_path, f'{file_name}_본문.xlsx')  # 최종 파일 경로\n",
    "error_log_path = os.path.join(base_path, f'{file_name}_error_log.txt')  # 에러 로그 파일 경로\n",
    "\n",
    "# 3. 파일 경로 생성 및 엑셀 파일 불러오기\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def fetch_article_details(article_url, row):\n",
    "    if (pd.notna(row['content']) and row['content'] != \"\") and (pd.notna(row['sub-title']) and row['sub-title'] != \"\"):\n",
    "        print(f\"이미 수집된 기사입니다. URL 접근을 건너뜁니다: {article_url}\")\n",
    "        return \"\", \"\"\n",
    "    \n",
    "    driver.get(article_url)\n",
    "    \n",
    "    try:        \n",
    "        # 기사 본문을 가져오기 위해 대기\n",
    "        WebDriverWait(driver, 3).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '#article_body'))\n",
    "        )\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 부제 추출 (div.ab_sub_headingline)\n",
    "        subtitle = \"\"\n",
    "        subtitle_tag = soup.find('div', class_='ab_sub_headingline')\n",
    "        subtitle = subtitle_tag.get_text(separator='\\n', strip=True) if subtitle_tag else ''\n",
    "\n",
    "        # 제거할 태그들 삭제\n",
    "        unwanted_tags = ['div#ad_art_content_btm', 'p.caption', 'div.series_list_wrap', 'div.player_area', 'div#ad_art_content_mid', 'div.ab_photo photo_center', 'body.template_special', 'div.html_photo_center', 'div.html_photo', 'div.divTable']\n",
    "        for tag in unwanted_tags:\n",
    "            for unwanted in soup.select(tag):\n",
    "                unwanted.decompose()\n",
    "\n",
    "        content = \"\\n\".join([p.get_text(separator='\\n', strip=True) for p in soup.select('#article_body')])  \n",
    "        if content.startswith('='):\n",
    "            content = \"'\" + content\n",
    "    except:\n",
    "        content = ''\n",
    "\n",
    "    return content, subtitle\n",
    "\n",
    "error_log = []\n",
    "\n",
    "def update_articles_with_details(input_df):\n",
    "    for index, row in input_df.iterrows():\n",
    "\n",
    "        print(f\"Fetching details for article {index + 1}/{len(input_df)}\")\n",
    "        content, subtitle = fetch_article_details(row['url'], row)\n",
    "        \n",
    "        # 기존 DataFrame에 새로운 값 업데이트\n",
    "        input_df.at[index, 'content'] = content\n",
    "        input_df.at[index, 'sub-title'] = subtitle\n",
    "\n",
    "        # 100개마다 파일 저장\n",
    "        if (index + 1) % 100 == 0:\n",
    "            df.to_excel(temp_output_path, index=False)\n",
    "            print(f\"임시 파일을 저장했습니다: {temp_output_path}\")\n",
    "\n",
    "    driver.quit()   \n",
    "    return input_df\n",
    "\n",
    "# 기사 링크에 들어가서 본문을 원본 DataFrame에 추가\n",
    "updated_df = update_articles_with_details(df)\n",
    "\n",
    "# 엑셀 파일로 저장\n",
    "updated_df = updated_df[~updated_df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "updated_df = updated_df[~updated_df['content'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "updated_df.to_excel(output_path, index=False)\n",
    "\n",
    "# 에러 로그 파일 저장\n",
    "if error_log:\n",
    "    with open(error_log_path, 'w') as f:\n",
    "        for url in error_log:\n",
    "            f.write(f\"{url}\\n\")\n",
    "    print(f\"에러 로그가 저장되었습니다: {error_log_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "from openpyxl.utils.exceptions import IllegalCharacterError  # 올바른 임포트 경로\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # 이미지 로드 비활성화\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 1. 출처와 파일명 변수 설정\n",
    "file_name = '파일명'  # 파일명 설정\n",
    "base_path = rf'경로'  # 경로 설정\n",
    "\n",
    "# 2. 임시 파일 경로와 최종 파일 경로 생성\n",
    "temp_output_path = os.path.join(base_path, f'{file_name}_temp.xlsx')\n",
    "output_path = os.path.join(base_path, f'{file_name}_본문.xlsx')  # 최종 파일 경로\n",
    "error_log_path = os.path.join(base_path, f'{file_name}_error_log.txt')  # 에러 로그 파일 경로\n",
    "\n",
    "# 3. 파일 경로 생성 및 엑셀 파일 불러오기\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# 에러 로그 초기화\n",
    "error_log = []\n",
    "\n",
    "def remove_illegal_characters(text):\n",
    "    if isinstance(text, str):\n",
    "        # Excel에서 허용하지 않는 유니코드 제어 문자 패턴\n",
    "        illegal_char_pattern = re.compile(\n",
    "            r'[\\x00-\\x08\\x0B\\x0C\\x0E-\\x1F]'\n",
    "        )\n",
    "        return illegal_char_pattern.sub('', text)\n",
    "    return text\n",
    "\n",
    "def fetch_article_details(article_url):\n",
    "    \"\"\"\n",
    "    기사 URL을 받아 본문과 부제를 추출하는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.get(article_url)\n",
    "        \n",
    "        # 기사 본문을 가져오기 위해 대기\n",
    "        WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, '#article_body'))\n",
    "        )\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "                # 부제 추출 (div.ab_sub_headingline)\n",
    "        subtitle = \"\"\n",
    "        subtitle_tag = soup.find('div', class_='ab_sub_headingline')\n",
    "        if subtitle_tag:\n",
    "            subtitle = subtitle_tag.get_text(separator='\\n', strip=True)\n",
    "            subtitle_tag.decompose() \n",
    "        else:\n",
    "            subtitle_tag = soup.find('div', class_='ab_subtitle')\n",
    "            if subtitle_tag:\n",
    "                subtitle = subtitle_tag.get_text(separator='\\n', strip=True)\n",
    "                subtitle_tag.decompose()         \n",
    "\n",
    "        # 제거할 태그들 삭제\n",
    "        unwanted_tags = [\n",
    "            'div#ad_art_content_btm', \n",
    "            'p.caption', \n",
    "            'div.series_list_wrap', \n",
    "            'div.player_area', \n",
    "            'div#ad_art_content_mid', \n",
    "            'div.ab_photo.photo_center', \n",
    "            'body.template_special',\n",
    "            'div.ab_related_article',\n",
    "            'section.related_link',\n",
    "            'div.divTable'\n",
    "        ]\n",
    "        for tag in unwanted_tags:\n",
    "            for unwanted in soup.select(tag):\n",
    "                unwanted.decompose()\n",
    "\n",
    "        # 본문 추출\n",
    "        content = \"\\n\".join([p.get_text(separator='\\n', strip=True) for p in soup.select('#article_body')])  \n",
    "        if content.startswith('='):\n",
    "            content = \"'\" + content\n",
    "\n",
    "        # 불법 문자 제거\n",
    "        content = remove_illegal_characters(content)\n",
    "        subtitle = remove_illegal_characters(subtitle)\n",
    "\n",
    "        return content, subtitle\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {article_url}: {e}\")\n",
    "        error_log.append(article_url)\n",
    "        return '', ''\n",
    "\n",
    "def update_articles_with_details(input_df):\n",
    "    \"\"\"\n",
    "    DataFrame을 순회하며 기사 상세 정보를 업데이트하는 함수.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for index, row in input_df.iterrows():\n",
    "            # # 'content' 컬럼이 이미 채워져 있는 경우 건너뜀\n",
    "            # if pd.notna(row['content']) and row['content'].strip() != \"\":\n",
    "            #     print(f\"이미 수집된 기사입니다. URL 접근을 건너뜁니다: {row['url']}\")\n",
    "            #     continue\n",
    "\n",
    "            print(f\"Fetching details for article {index + 1}/{len(input_df)}: {row['url']}\")\n",
    "            content, subtitle = fetch_article_details(row['url'])\n",
    "            \n",
    "            # 기존 DataFrame에 새로운 값 업데이트\n",
    "            input_df.at[index, 'content'] = content\n",
    "            input_df.at[index, 'sub-title'] = subtitle\n",
    "\n",
    "            # 100개마다 파일 저장\n",
    "            if (index + 1) % 100 == 0:\n",
    "                try:\n",
    "                    # 임시 파일 저장 전에 불법 문자 제거\n",
    "                    input_df.to_excel(temp_output_path, index=False)\n",
    "                    print(f\"임시 파일을 저장했습니다: {temp_output_path}\")\n",
    "                except IllegalCharacterError as e:\n",
    "                    print(f\"임시 파일 저장 중 불법 문자가 발견되었습니다: {e}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"임시 파일 저장 중 다른 오류가 발생했습니다: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    return input_df\n",
    "\n",
    "# 기사 링크에 들어가서 본문을 원본 DataFrame에 추가\n",
    "updated_df = update_articles_with_details(df)\n",
    "\n",
    "# 불법 문자 제거 (추가적으로 안전을 위해 모든 객체형 컬럼에 적용)\n",
    "object_columns = updated_df.select_dtypes(['object']).columns\n",
    "for col in object_columns:\n",
    "    updated_df[col] = updated_df[col].apply(remove_illegal_characters)\n",
    "\n",
    "# 엑셀 파일로 저장\n",
    "try:\n",
    "    # 특정 키워드가 포함된 기사를 필터링\n",
    "    filtered_df = updated_df[\n",
    "        ~updated_df['title'].str.contains('블락비|나인뮤지스', case=False, na=False) &\n",
    "        ~updated_df['content'].str.contains('블락비|나인뮤지스', case=False, na=False)\n",
    "    ]\n",
    "    filtered_df.to_excel(output_path, index=False)\n",
    "    print(f\"최종 파일이 저장되었습니다: {output_path}\")\n",
    "except IllegalCharacterError as e:\n",
    "    print(f\"엑셀 저장 중 불법 문자가 발견되었습니다: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"엑셀 저장 중 다른 오류가 발생했습니다: {e}\")\n",
    "\n",
    "# 에러 로그 파일 저장\n",
    "if error_log:\n",
    "    with open(error_log_path, 'w', encoding='utf-8') as f:\n",
    "        for url in error_log:\n",
    "            f.write(f\"{url}\\n\")\n",
    "    print(f\"에러 로그가 저장되었습니다: {error_log_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
