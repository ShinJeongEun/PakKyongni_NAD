{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 동아디지털아카이브(1996년 10월 14일 이전 기사, 지면 기사) 목록, 본문\n",
    "# 1996년 10월 14일 이후 기사는 홈페이지와 디지털아카이브 두 군데에 아카이빙되어 있으므로, 이 코드는 1996년 10월 14일 이전에만 사용할 것.\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 필요시 활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# 사용자 정보\n",
    "user_id = 'id'\n",
    "user_pw = 'pw'\n",
    "\n",
    "# 검색어\n",
    "keywords = [\n",
    "    \"검색어\",\n",
    "]\n",
    "# 검색 기간 설정\n",
    "start_date = \"20240601\"\n",
    "end_date = \"20240831\"\n",
    "\n",
    "collected_urls = set()\n",
    "\n",
    "# 로그인 페이지로 이동\n",
    "driver.get('https://secure.donga.com/membership/login.php?gourl=https%3A%2F%2Fwww.donga.com')\n",
    "\n",
    "# ID와 PW 입력 및 로그인\n",
    "driver.find_element(By.NAME, 'email').send_keys(user_id)\n",
    "driver.find_element(By.NAME, 'bpw').send_keys(user_pw)\n",
    "driver.find_element(By.ID, 'loginbtn').click()\n",
    "\n",
    "# 로그인 후 대기\n",
    "time.sleep(1)\n",
    "\n",
    "# 뉴스 라이브러리 페이지로 이동 및 검색어 리스트 설정\n",
    "base_url = \"https://www.donga.com/archive/newslibrary\"\n",
    "\n",
    "\n",
    "# 각 검색어에 대해 기사 검색 및 정보 수집\n",
    "articles = []\n",
    "\n",
    "def parse_articles():\n",
    "    page_source = driver.page_source\n",
    "    soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "    articles = soup.find_all(\"div\", class_=\"tit_cont\")\n",
    "    return articles\n",
    "\n",
    "\n",
    "def extract_article_info(article):\n",
    "    url_tag = article.find(\"a\")\n",
    "    if not url_tag:\n",
    "        return None\n",
    "\n",
    "    title_tag = url_tag.find(\"span\", class_=\"titie\")\n",
    "    date_tag = url_tag.find(\"span\", class_=\"date\")\n",
    "    position_tag = url_tag.find(\"span\", class_=\"position\")\n",
    "\n",
    "    if not title_tag or not date_tag or not position_tag:\n",
    "        return None\n",
    "\n",
    "    title = title_tag.text.strip()\n",
    "    url = url_tag[\"href\"]\n",
    "    date = date_tag.text.strip()\n",
    "    position = position_tag.text.strip()\n",
    "\n",
    "    position_parts = position.split(\" \")\n",
    "    page = position_parts[1] if len(position_parts) >= 2 else position\n",
    "    section = \" \".join(position_parts[2:]) if len(position_parts) >= 2 else \"\"\n",
    "\n",
    "        # 중복 URL 체크: 이미 수집된 URL이면 None 반환\n",
    "    if url in collected_urls:\n",
    "        return None\n",
    "\n",
    "    # 새로운 URL이면 집합에 추가\n",
    "    collected_urls.add(url)\n",
    "\n",
    "    return (title, url, date, page, section)\n",
    "\n",
    "for keyword in keywords:\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"{keyword} - {page_number} 건을 처리 중입니다...\")\n",
    "            driver.get(f\"{base_url}?p={page_number}&query={keyword}&range=1&search_date=&sdate={start_date}&edate={end_date}&sec_top=&sorting=2&option=1&l=20\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            articles = parse_articles()\n",
    "            if not articles:\n",
    "                break\n",
    "            \n",
    "            for index, article in enumerate(articles):\n",
    "                info = extract_article_info(article)\n",
    "                if info:\n",
    "                    title, url, date, page, section = info\n",
    "\n",
    "                    # 링크 클릭하여 기사 페이지로 이동\n",
    "                    driver.get(url)\n",
    "                    time.sleep(2)  # 페이지가 로드될 때까지 대기\n",
    "                    article_page = driver.page_source\n",
    "                    article_soup = BeautifulSoup(article_page, \"html.parser\")\n",
    "\n",
    "                    # 원문, 독음 추출 및 입력\n",
    "                    original_content_tag = article_soup.find(\"div\", id=\"article_origin\")\n",
    "                    transfer_content_tag = article_soup.find(\"div\", id=\"article_transfer\")\n",
    "                    ori_content = original_content_tag.find(\"div\", class_=\"article_txt\").get_text(separator=\"\\n\").strip().replace('\\n', ' ') if original_content_tag else \"\"\n",
    "                    trans_content = transfer_content_tag.find(\"div\", class_=\"article_txt\").get_text(separator=\"\\n\").strip().replace('\\n', ' ') if transfer_content_tag else \"\"\n",
    "                    if trans_content.startswith('='):\n",
    "                       trans_content = \"'\" + trans_content\n",
    "\n",
    "                    # 글쓴이 추출\n",
    "                    writer_tag = transfer_content_tag.find(\"span\", class_=\"reporter\")\n",
    "\n",
    "                    # 글쓴이 입력\n",
    "                    writer = writer_tag.text.strip() if writer_tag else \"\"\n",
    "                    \n",
    "                    articles.append({\n",
    "                        \"media\": \"동아일보\",\n",
    "                        \"title\": title,\n",
    "                        \"date\": date,\n",
    "                        \"section\": section,\n",
    "                        \"article_type\": '',\n",
    "                        'writer': writer,\n",
    "                        \"page\": page,\n",
    "                        \"url\": url,\n",
    "                        'content': trans_content\n",
    "                        })\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{page_number} 페이지 처리 중 오류 발생: {e}\")\n",
    "\n",
    "        # 다음 페이지로 이동\n",
    "        page_number += 20\n",
    "        next_page_url = f\"{base_url}?p={page_number}&query={keyword}&range=1&search_date=&sdate=&edate=&sec_top=&sorting=2&option=1&l=20\"\n",
    "        driver.get(next_page_url)\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 더 이상 페이지가 없으면 루프 종료\n",
    "        if \"현재 페이지의 기사목록이 없습니다.\" in driver.page_source:\n",
    "            print(\"더 이상 다음 페이지가 없습니다.\")\n",
    "            break\n",
    "\n",
    "# 데이터프레임 생성 및 중복 기사 제거\n",
    "df = pd.DataFrame(articles)\n",
    "df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "# 엑셀 파일 저장 경로 설정\n",
    "output_file = os.path.join(os.getcwd(), f'1동아일보 {start_date}~{end_date}.xlsx')\n",
    "\n",
    "# 데이터프레임 엑셀 저장\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"파일 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 홈페이지 뉴스(1996년 10월 14일 이후 기사) 목록\n",
    "\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 필요시 활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--start-maximized\")\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 웹 드라이버 초기화\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# 검색어\n",
    "keywords = [\n",
    "    \"검색어\",\n",
    "]\n",
    "\n",
    "# 검색 기간 설정\n",
    "start_date = \"20000101\"\n",
    "end_date = \"20240531\"\n",
    "collected_urls = set()\n",
    "articles = []  # 모든 기사 정보를 저장할 리스트\n",
    "\n",
    "# 각 키워드에 대해 기사 검색 및 정보 수집\n",
    "for keyword in keywords:\n",
    "    page_number = 1\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            print(f\"{keyword} 기사 {page_number} 건를 처리 중입니다...\")\n",
    "            driver.get(f\"https://www.donga.com/news/search?p={page_number}&query={keyword}&sorting=1&check_news=91&sorting=1&search_date=5&v1={start_date}&v2={end_date}&more=1\")\n",
    "            time.sleep(2)\n",
    "\n",
    "            # 페이지 소스 파싱\n",
    "            soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "            li_elements = soup.find(\"ul\", class_=\"row_list\").find_all(\"li\", class_=\"\") if soup.find(\"ul\", class_=\"row_list\") else []\n",
    "\n",
    "            if not li_elements:  # li 태그가 없는 경우 루프 종료\n",
    "                break  \n",
    "\n",
    "            for li in li_elements:\n",
    "                article = li.find(\"div\", class_=\"news_body\")\n",
    "                if article:\n",
    "                    title_tag = article.find(\"h4\", class_=\"tit\")\n",
    "                    url_tag = title_tag.find(\"a\") if title_tag else None\n",
    "                    date_tag = article.find(\"span\", class_=\"date\")\n",
    "\n",
    "                    if not title_tag or not url_tag or not date_tag:\n",
    "                        continue\n",
    "\n",
    "                    title = title_tag.get_text(strip=True)\n",
    "                    url = url_tag[\"href\"]\n",
    "                    date = date_tag.get_text(strip=True)\n",
    "\n",
    "                    # 중복 URL 체크\n",
    "                    if url in collected_urls:\n",
    "                        continue\n",
    "                    collected_urls.add(url)\n",
    "\n",
    "                    # 기사 정보를 articles 리스트에 추가\n",
    "                    articles.append({\n",
    "                        \"media\": \"동아일보\",\n",
    "                        \"title\": title,\n",
    "                        \"date\": date,\n",
    "                        \"section\": '', \n",
    "                        \"article_type\": '',\n",
    "                        'writer': '',  \n",
    "                        \"page\": '',  \n",
    "                        \"url\": url,\n",
    "                        'content': ''  \n",
    "                    })\n",
    "\n",
    "            # 유효한 기사가 10개 미만이면 루프 종료\n",
    "            if len(li_elements) < 10:\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"{page_number} 페이지 처리 중 오류 발생: {e}\")\n",
    "\n",
    "        page_number += 10\n",
    "\n",
    "# 데이터프레임 생성 및 중복 기사 제거\n",
    "df = pd.DataFrame(articles)\n",
    "df = df.drop_duplicates(subset=['url'], keep='first')\n",
    "\n",
    "# 엑셀 파일 저장 경로 설정\n",
    "output_file = os.path.join(os.getcwd(), f'1동아일보 {start_date}~{end_date}.xlsx')\n",
    "\n",
    "# 데이터프레임 엑셀 저장\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(\"파일 저장 완료!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 추출\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 1. 출처와 파일명 변수 설정\n",
    "file_name = '파일명'  # 파일명 설정\n",
    "base_path = rf'경로'  # 경로 설정\n",
    "\n",
    "# 2. 임시 파일 경로와 최종 파일 경로 생성\n",
    "temp_output_path = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "# 임시 파일 에러를 대비해 본 파일에 덮어쓴다.\n",
    "output_path = os.path.join(base_path, f'{file_name}_본문.xlsx')  # 최종 파일 경로\n",
    "error_log_path = os.path.join(base_path, 'error_log.txt')  # 오류 로그 경로\n",
    "\n",
    "# 3. 파일 경로 생성 및 엑셀 파일 불러오기\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# Selenium 헤드리스 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# ChromeDriver 설정 없이 크롬 브라우저 실행\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "# URL에서 본문과 서브 타이틀을 스크랩하는 함수 정의\n",
    "def get_content_and_subtitle_from_url(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "        # 서브 타이틀 추출 (h2.sub_tit)\n",
    "        subtitle_tag = soup.find('h2', class_='sub_tit')\n",
    "        subtitle = \"\"\n",
    "        if subtitle_tag:\n",
    "            subtitle = subtitle_tag.get_text(separator='\\n', strip=True)\n",
    "            subtitle_tag.decompose()  # 부제를 본문에서 제거\n",
    "\n",
    "        # 본문 추출\n",
    "        article_body = None\n",
    "        if url.startswith('https://www.donga.com/news/'):\n",
    "            article_body = soup.find('section', class_='news_view')\n",
    "        else:\n",
    "            article_body = soup.find('div', class_='article_txt')\n",
    "\n",
    "        if article_body:\n",
    "            # 제거할 태그와 클래스 리스트\n",
    "            tags_to_remove = [\n",
    "                ('figure', 'img_cont'),  \n",
    "                ('figcaption', None)    \n",
    "            ]\n",
    "\n",
    "            # 태그를 반복문으로 순회하면서 제거\n",
    "            for tag, class_name in tags_to_remove:\n",
    "                if class_name:\n",
    "                    elements = article_body.find_all(tag, class_=class_name)\n",
    "                else:\n",
    "                    elements = article_body.find_all(tag)\n",
    "                \n",
    "                for element in elements:\n",
    "                    element.decompose()\n",
    "\n",
    "            # 본문 텍스트 추출\n",
    "            content = article_body.get_text(separator='\\n', strip=True)\n",
    "        else:\n",
    "            content = \"\"\n",
    "\n",
    "        return content, subtitle\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 진행 중인 기사 스크래핑 상태를 트래킹할 리스트와 로그 파일\n",
    "error_log = []\n",
    "total_articles = len(df)\n",
    "\n",
    "# 각 행의 URL에서 본문과 서브 타이틀 내용을 스크랩하여 content와 sub-title 컬럼에 저장합니다.\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing {index + 1}/{total_articles}: {row['url']}\")\n",
    "    content, subtitle = get_content_and_subtitle_from_url(row['url'])\n",
    "\n",
    "    if content is not None and content.startswith('='):\n",
    "        content = \"'\" + content\n",
    "\n",
    "    df.at[index, 'content'] = content if content is not None else \"\"\n",
    "    df.at[index, 'sub-title'] = subtitle  # 서브 타이틀이 없으면 빈 문자열로 저장\n",
    "\n",
    "    # 100개마다 파일 저장\n",
    "    if (index + 1) % 100 == 0:\n",
    "        df.to_excel(temp_output_path, index=False)\n",
    "        print(f\"임시 파일을 저장했습니다: {temp_output_path}\")\n",
    "\n",
    "# 웹드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 변경된 데이터를 파일로 저장합니다.\n",
    "df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "df = df[~df['content'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "df.to_excel(output_path, index=False)\n",
    "\n",
    "print(\"본문 내용을 성공적으로 스크랩하여 저장했습니다.\")\n",
    "\n",
    "# 에러 로그 파일 저장\n",
    "if error_log:\n",
    "    with open(error_log_path, 'w') as f:\n",
    "        for url in error_log:\n",
    "            f.write(f\"{url}\\n\")\n",
    "    print(f\"에러 로그가 저장되었습니다: {error_log_path}\")\n",
    "\n",
    "\n",
    "      \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
