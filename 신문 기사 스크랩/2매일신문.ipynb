{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 목록 저장. 매일신문은 시간을 좀 오래 기다려야 뜨는 일이 많음.\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "import pandas as pd\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 크롬 드라이버 옵션 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # 필요시 활성화\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "chrome_options.add_argument(\"--blink-settings=imagesEnabled=false\")  # 이미지 로드 비활성화\n",
    "chrome_options.add_argument(\"--disable-extensions\")  # 확장 프로그램 비활성화\n",
    "chrome_options.add_argument(\"--disable-gpu\")  # GPU 가속 비활성화\n",
    "\n",
    "# 크롬 드라이버 설정\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "driver.maximize_window()\n",
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "# 키워드 목록과 날짜 범위 설정\n",
    "keywords = [\n",
    "    \"검색어\",\n",
    "]\n",
    "start_date = \"2024-06-01\"\n",
    "end_date = \"2024-08-31\"\n",
    "\n",
    "collected_urls = set()\n",
    "all_articles = []\n",
    "\n",
    "def search_and_save_articles(keyword):\n",
    "    url = \"https://search.imaeil.com/RSA/front_new/Search.jsp\"\n",
    "    driver.get(url)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # 검색어 입력\n",
    "    search_box = driver.find_element(By.ID, 'qt')\n",
    "    search_box.send_keys(keyword)\n",
    "    search_box.submit()\n",
    "\n",
    "    try:\n",
    "        more_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(@title, \"더보기\")]')))\n",
    "        more_button.click()\n",
    "        print(f\"'더보기' 버튼 클릭 완료.\")\n",
    "    except Exception as e:\n",
    "        print(f\"'더보기' 버튼 클릭 실패 또는 존재하지 않음: {e}\")\n",
    "\n",
    "    articles = []\n",
    "\n",
    "    def extract_articles():\n",
    "        try:\n",
    "            wait.until(EC.presence_of_element_located((By.ID, 'board_sc')))\n",
    "            time.sleep(5)  # 'board_sc' 요소가 나타날 때까지 대기\n",
    "        except Exception as e:\n",
    "            print(f\"Error waiting for articles section: {e}\")\n",
    "            return False\n",
    "    \n",
    "        soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "        articles_section = soup.find(\"div\", id=\"board_sc\")  # 기사 섹션 확인\n",
    "        time.sleep(5)\n",
    "        \n",
    "        # 기사 섹션이 제대로 있는지 확인\n",
    "        if not articles_section:\n",
    "            print(\"No articles section found!\")\n",
    "            return False\n",
    "\n",
    "        articles_list = articles_section.find_all(\"li\", class_=\"txt\")\n",
    "        if not articles_list:\n",
    "            print(\"No articles found!\")\n",
    "            return False\n",
    "\n",
    "        # 기사 정보 추출\n",
    "        for item in articles_list:\n",
    "            try:\n",
    "                # 제목 및 링크 추출\n",
    "                title_tag = item.find(\"a\", title=True)\n",
    "                if title_tag:\n",
    "                    title = title_tag[\"title\"].strip()\n",
    "                    url = title_tag[\"href\"]\n",
    "                else:\n",
    "                    title = \"\"\n",
    "                    url = \"\"\n",
    "\n",
    "                # 날짜는 해당 아이템의 다음 li.txt2에서 추출\n",
    "                date_element = item.find_next_sibling(\"li\", class_=\"txt2\").find(\"span\", class_=\"wGun\")\n",
    "                date = date_element.text.strip().split()[0] if date_element else \"\"\n",
    "\n",
    "                # 시작 날짜 이전 기사면 종료\n",
    "                if date < start_date:\n",
    "                    print(f\"기사가 시작 날짜 이전입니다. 스크래핑을 중단합니다: {date}\")\n",
    "                    return False  # 루프 종료\n",
    "\n",
    "                # 종료 날짜 이후의 기사는 무시\n",
    "                if date > end_date:\n",
    "                    continue  # 범위 밖의 기사는 스킵\n",
    "\n",
    "                if url in collected_urls:\n",
    "                    continue\n",
    "                collected_urls.add(url)\n",
    "\n",
    "                # 기사 데이터 추가\n",
    "                articles.append({\n",
    "                    'media': '매일신문',\n",
    "                    'title': title,\n",
    "                    'date': date,\n",
    "                    'section': '',\n",
    "                    'article_type': '',\n",
    "                    'writer': '',\n",
    "                    'page': '',\n",
    "                    'url': url,\n",
    "                    'content': '' \n",
    "                })\n",
    "                print(f\"Added article: {title} | {date} | {url}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing article: {e}\")\n",
    "        return True\n",
    "\n",
    "    def scrape_page_group():\n",
    "        page_num = 1\n",
    "        while True:\n",
    "            try:\n",
    "                if page_num == 1:\n",
    "                    # 첫 페이지는 이미 로드되었으므로 추가 클릭이 필요 없음\n",
    "                    pass\n",
    "                else:\n",
    "                    page_link = wait.until(EC.element_to_be_clickable((By.XPATH, f'//a[text()=\"{page_num}\"]')))\n",
    "                    page_link.click()\n",
    "\n",
    "                if not extract_articles():\n",
    "                    break  # 날짜 조건에 의해 중단\n",
    "\n",
    "                # >\n",
    "                if page_num % 10 == 0:  # Click \">\" button after every 10 pages\n",
    "                    try:\n",
    "                        next_button = wait.until(EC.element_to_be_clickable((By.XPATH, '//a[contains(@onclick, \"clickFldSet\") and contains(text(), \">\")]')))\n",
    "                        next_button.click()\n",
    "                        time.sleep(2)  # 필요 시 조정\n",
    "                        page_num = 0  # 다음 그룹의 페이지 번호 초기화\n",
    "                    except Exception as e:\n",
    "                        print(f\"'>' 버튼 클릭 실패: {e}\")\n",
    "                        break\n",
    "\n",
    "                page_num += 1\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Page navigation error: {e}\")\n",
    "                break\n",
    "        \n",
    "        return articles\n",
    "    \n",
    "    scrape_page_group()\n",
    "\n",
    "    print(f\"키워드 '{keyword}'에 대해 수집된 기사 수: {len(articles)}\")\n",
    "\n",
    "    return articles\n",
    "\n",
    "# 모든 키워드에 대해 함수 호출\n",
    "for keyword in keywords:\n",
    "    print(f\"\\n키워드 검색 시작: {keyword}\")\n",
    "    articles = search_and_save_articles(keyword)\n",
    "    if not articles:\n",
    "        print(f\"키워드 '{keyword}'에 대한 기사가 더 없습니다. 다음 키워드로 이동합니다.\")\n",
    "        continue  # 다음 키워드로 이동\n",
    "\n",
    "    all_articles.extend(articles)\n",
    "\n",
    "# 기사 추출 함수 호출\n",
    "\n",
    "    # 데이터프레임으로 변환\n",
    "    df = pd.DataFrame(all_articles)\n",
    "    df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]        \n",
    "    df.to_excel(f'2매일신문 {start_date}~{end_date}.xlsx', index=False)\n",
    "    time.sleep(2)  # 각 키워드 사이에 잠시 대기    \n",
    "\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 본문 추출\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import TimeoutException\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Selenium 헤드리스 설정\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "\n",
    "# 1. 출처와 파일명 변수 설정\n",
    "file_name = '파일명'  # 파일명 설정\n",
    "base_path = rf'경로'  # 경로 설정\n",
    "\n",
    "# 2. 임시 파일 경로와 최종 파일 경로 생성\n",
    "temp_output_path = os.path.join(base_path, f'{file_name}temp.xlsx')\n",
    "output_path = os.path.join(base_path, f'{file_name}_본문.xlsx')  # 최종 파일 경로\n",
    "error_log_path = os.path.join(base_path, f'{file_name}_error_log.txt')  # 에러 로그 파일 경로\n",
    "\n",
    "# 3. 파일 경로 생성 및 엑셀 파일 불러오기\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# ChromeDriver 설정 없이 크롬 브라우저 실행\n",
    "driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "def wait_for_article_body(wait, class_names):\n",
    "    \"\"\"주어진 클래스 이름 중 첫 번째로 매칭된 요소를 반환합니다.\"\"\"\n",
    "    for class_name in class_names:\n",
    "        try:\n",
    "            return wait.until(EC.presence_of_element_located((By.CLASS_NAME, class_name)))\n",
    "        except TimeoutException:\n",
    "            continue \n",
    "    return None\n",
    "\n",
    "def get_content_and_subtitle_from_url(url):\n",
    "    try:\n",
    "        driver.get(url)\n",
    "\n",
    "        # WebDriverWait 설정\n",
    "        wait = WebDriverWait(driver, 5)\n",
    "\n",
    "        # 기사 본문을 찾기 위한 클래스 이름 리스트\n",
    "        class_names = ['article_content', 'articlebody', 'newsct_article', 'article-txt', 'story-news', 'content01']\n",
    "\n",
    "        # 첫 번째로 매칭된 요소 찾기\n",
    "        article_body_element = wait_for_article_body(wait, class_names)\n",
    "\n",
    "        if article_body_element:\n",
    "            # 페이지 소스에서 BeautifulSoup으로 변환\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            # 서브 타이틀 추출 및 제거\n",
    "            subtitle = \"\"\n",
    "            subtitle_tag = soup.find('p', class_='subtitle')\n",
    "            if subtitle_tag:\n",
    "                subtitle = subtitle_tag.get_text(separator='\\n', strip=True)\n",
    "                subtitle_tag.decompose()  # 서브 타이틀 제거\n",
    "\n",
    "            # 'figure', 'table', 'div' 태그 및 기타 불필요한 태그 제거\n",
    "            for tag in soup.find_all(['figure', 'table', 'div'], class_=['img_center', 'article-figure', 'nbd_table', 'comp-box photo-group', 'article-img', 'img-con', 'chn_box']):\n",
    "                tag.decompose()\n",
    "\n",
    "            # 본문에서 <span data-type=\"ore\"> 태그를 제거하고 안의 텍스트만 남김\n",
    "            for span_tag in soup.find_all('span', {'data-type': 'ore'}):\n",
    "                span_tag.unwrap()  # <span> 태그를 제거하고 텍스트만 유지\n",
    "\n",
    "            # 본문 내용 추출\n",
    "            article_body = soup.find(class_=class_names)\n",
    "            if article_body:\n",
    "                content = article_body.get_text(separator='\\n', strip=True)\n",
    "            else:\n",
    "                print(f\"본문을 찾지 못했습니다: {url}\")\n",
    "                content = \"\"\n",
    "\n",
    "            # 저작권 정보 제거\n",
    "            txtcopyright = soup.find('p', class_='txt-copyright')\n",
    "            if txtcopyright:\n",
    "                txtcopyright.decompose()\n",
    "\n",
    "            return content, subtitle\n",
    "        else:\n",
    "            return \"\", \"\"\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching URL {url}: {e}\")\n",
    "        return None, None\n",
    "\n",
    "# 진행 중인 기사 스크래핑 상태를 트래킹할 리스트와 로그 파일\n",
    "error_log = []\n",
    "total_articles = len(df)\n",
    "\n",
    "# 각 행의 URL에서 본문과 서브 타이틀 내용을 스크랩하여 content와 sub-title 컬럼에 저장합니다.\n",
    "for index, row in df.iterrows():\n",
    "    print(f\"Processing {index + 1}/{total_articles}: {row['url']}\")\n",
    "    content, subtitle = get_content_and_subtitle_from_url(row['url'])\n",
    "    if content is None:\n",
    "        error_log.append(row['url'])\n",
    "    df.at[index, 'content'] = content\n",
    "    df.at[index, 'sub-title'] = subtitle  # 서브 타이틀 저장\n",
    "\n",
    "    # 100개마다 파일 저장\n",
    "    if (index + 1) % 50 == 0:\n",
    "        df.to_excel(temp_output_path, index=False)\n",
    "        print(f\"임시 파일을 저장했습니다: {temp_output_path}\")\n",
    "\n",
    "# 웹드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 에러 로그 파일 저장\n",
    "if error_log:\n",
    "    with open(error_log_path, 'w') as f:\n",
    "        for url in error_log:\n",
    "            f.write(f\"{url}\\n\")\n",
    "    print(f\"에러 로그가 저장되었습니다: {error_log_path}\")\n",
    "\n",
    "# 변경된 데이터를 파일로 저장합니다.\n",
    "df = df[~df['title'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "df = df[~df['content'].str.contains('블락비|나인뮤지스', case=False, na=False)]\n",
    "df.to_excel(output_path, index=False)\n",
    "print(\"본문 내용을 성공적으로 스크랩하여 저장했습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
