{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
        "\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.common.exceptions import ElementNotInteractableException, NoSuchElementException\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "# 브라우저 옵션 설정\n",
    "options = Options()\n",
    "service = Service()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--headless\")  # 브라우저 창을 띄우지 않고 실행\n",
    "options.add_argument(\"--disable-gpu\")  # GPU 비활성화 (headless 모드에서 권장)\n",
    "\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "\n",
    "# 베이스 URL 설정\n",
    "base_url = \"https://newslibrary.naver.com/search/searchByKeyword.naver\"\n",
    "\n",
    "# 검색어 설정\n",
    "search_queries = [\"검색어어\"]\n",
    "\n",
    "# 모든 기사 정보를 담을 리스트\n",
    "all_articles = []\n",
    "# 중복 체크를 위한 URL 집합\n",
    "seen_urls = set()\n",
    "\n",
    "for search_query in search_queries:\n",
    "    page_num = 1\n",
    "    while True:\n",
    "        print(f\"검색어 '{search_query}'의 {page_num}페이지 검색 중...\")\n",
    "\n",
    "        # 페이지 URL 설정\n",
    "        url = f\"https://newslibrary.naver.com/search/searchByKeyword.naver#%7B%22mode%22%3A1%2C%22sort%22%3A0%2C%22trans%22%3A%221%22%2C%22pageSize%22%3A10%2C%22keyword%22%3A%22{search_query}%22%2C%22status%22%3A%22success%22%2C%22startIndex%22%3A1%2C%22page%22%3A{page_num}%2C%22startDate%22%3A%221955-01-01%22%2C%22endDate%22%3A%221999-12-31%22%7D\"\n",
    "        \n",
    "        # 사이트로 이동\n",
    "        driver.get(url)\n",
    "\n",
    "        # 페이지 로딩을 위한 대기 시간\n",
    "        time.sleep(2)\n",
    "\n",
    "        # 페이지 소스 가져오기\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # BeautifulSoup를 사용하여 HTML 파싱\n",
    "        soup = BeautifulSoup(page_source, \"html.parser\")\n",
    "\n",
    "        # 기사 목록 가져오기\n",
    "        articles = soup.find_all(\"li\")\n",
    "\n",
    "        # 기사 정보가 없으면 종료\n",
    "        if not articles:\n",
    "            break\n",
    "\n",
    "        # 기사 정보 추출\n",
    "        for article in articles:\n",
    "            try:\n",
    "                date = article.find(\"li\", class_=\"date\").text.strip()\n",
    "                media_name = article.find(\"li\", class_=\"first\").text.strip()\n",
    "                page = article.find(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").text.strip()\n",
    "                article_type = article.find(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").text.strip()\n",
    "                section = article.find(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").find_next_sibling(\"li\").text.strip()\n",
    "                title_tag = article.find(\"h3\").find(\"a\")\n",
    "                title = title_tag.text.strip()\n",
    "                date = article.find(\"li\", class_=\"date\").text.strip()\n",
    "                url = title_tag.get('href')\n",
    "                full_url = f\"https://newslibrary.naver.com{url}\"\n",
    "                \n",
    "                # articleId 추출\n",
    "                article_id_match = re.search(r'articleId=(\\d{19})', full_url)\n",
    "                article_id = article_id_match.group(1) if article_id_match else None\n",
    "\n",
    "                # 중복 기사 제외\n",
    "                if full_url not in seen_urls:\n",
    "                    all_articles.append({\n",
    "                        \"media\": media_name,\n",
    "                        \"page\": page,\n",
    "                        \"section\": section,\n",
    "                        \"article_type\": article_type,\n",
    "                        \"title\": title,\n",
    "                        \"date\": date,\n",
    "                        \"url\": full_url,\n",
    "                        \"articleId\": article_id,\n",
    "                        \"search_query\": search_query,\n",
    "                        'author': '',\n",
    "                        'content': '' \n",
    "                    })\n",
    "                    seen_urls.add(full_url)\n",
    "            except AttributeError:\n",
    "                # 필요한 정보가 누락된 경우\n",
    "                continue\n",
    "\n",
    "        try:\n",
    "            # 다음 페이지 버튼 확인 및 클릭\n",
    "            if page_num % 10 == 0:\n",
    "                next_button = driver.find_element(By.CLASS_NAME, 'next')\n",
    "                next_button.click()\n",
    "                page_num += 1\n",
    "                time.sleep(2)  # 페이지 로드 대기\n",
    "            else:\n",
    "                next_buttons = driver.find_elements(By.CSS_SELECTOR, \"div#paginate a\")\n",
    "                next_button_clicked = False\n",
    "                for button in next_buttons:\n",
    "                    if button.text == str(page_num + 1):\n",
    "                        button.click()\n",
    "                        page_num += 1\n",
    "                        time.sleep(2)  # 페이지 로드 대기\n",
    "                        next_button_clicked = True\n",
    "                        break\n",
    "                if not next_button_clicked:\n",
    "                    print(f\"더 이상 다음 페이지가 없습니다. 종료합니다. (페이지 {page_num})\")\n",
    "                    break\n",
    "        except (ElementNotInteractableException, NoSuchElementException):\n",
    "            print(f\"페이지 {page_num}에서 다음 페이지로 넘어갈 수 없습니다. 종료합니다.\")\n",
    "            break\n",
    "\n",
    "# 드라이버 종료\n",
    "driver.quit()\n",
    "\n",
    "# 결과 출력\n",
    "for article in all_articles:\n",
    "    print(f\"검색어: {article['search_query']}, 매체명: {article['media']}, 지면: {article['page']}, 섹션: {article['section']}, 글 종류: {article['article_type']}, 제목: {article['title']}, 날짜: {article['date']}, 주소: {article['url']}, 아티클ID: {article['articleId']}\")\n",
    "\n",
    "# 데이터프레임으로 변환\n",
    "df = pd.DataFrame(all_articles)\n",
    "\n",
    "# 엑셀 파일 저장 경로 설정\n",
    "output_file = os.path.join(os.getcwd(), \"네이버 뉴스 라이브러리 기사 목록록.xlsx\")\n",
    "\n",
    "# 엑셀 파일로 저장\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"기사 정보가 {output_file} 파일로 저장되었습니다.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#본문 추출 방법 아티클 아이디와 기사 주소가 필요함\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "# 기본 설정\n",
    "base_path = rf\"경로\"\n",
    "file_name = \"목록 파일명\"\n",
    "\n",
    "# 파일 경로 설정\n",
    "input_file = os.path.join(base_path, f'{file_name}.xlsx')\n",
    "\n",
    "# Excel 파일 읽기\n",
    "df = pd.read_excel(input_file, engine='openpyxl')\n",
    "\n",
    "# \"본문\" 컬럼 추가\n",
    "df[\"본문\"] = \"\"\n",
    "\n",
    "# articleId 컬럼을 문자열로 변환\n",
    "df['articleId'] = df['articleId'].astype(str)\n",
    "\n",
    "# API 요청을 위한 설정\n",
    "url = \"https://newslibrary.naver.com/api/article/detail/json\"\n",
    "headers = {\n",
    "    \"accept\": \"*/*\",\n",
    "    \"accept-language\": \"ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7\",\n",
    "    \"content-type\": \"application/x-www-form-urlencoded; charset=UTF-8\",\n",
    "    \"priority\": \"u=1, i\",\n",
    "    \"sec-ch-ua\": \"\\\"Google Chrome\\\";v=\\\"125\\\", \\\"Chromium\\\";v=\\\"125\\\", \\\"Not.A/Brand\\\";v=\\\"24\\\"\",\n",
    "    \"sec-ch-ua-mobile\": \"?0\",\n",
    "    \"sec-ch-ua-platform\": \"\\\"Windows\\\"\",\n",
    "    \"sec-fetch-dest\": \"empty\",\n",
    "    \"sec-fetch-mode\": \"cors\",\n",
    "    \"sec-fetch-site\": \"same-origin\",\n",
    "    \"referer\": \"\"\n",
    "}\n",
    "\n",
    "# 각 행에 대해 API 요청 수행 및 응답 저장\n",
    "for index, row in df.iterrows():\n",
    "    article_id = row['articleId']\n",
    "    article_url = row['url']\n",
    "    \n",
    "    data = {\n",
    "        \"articleId\": article_id,\n",
    "        \"detailCode\": \"1001000001000000000001101100000000000000000\",\n",
    "        \"urlKey\": \"articleDetail\",\n",
    "        \"viewID\": \"app_articleDetail\",\n",
    "        \"requestID\": \"4\",\n",
    "        \"target\": \"viewer\"\n",
    "    }\n",
    "    \n",
    "    # referrer 설정\n",
    "    headers[\"referer\"] = article_url\n",
    "\n",
    "    # POST 요청\n",
    "    response = requests.post(url, headers=headers, data=data)\n",
    "\n",
    "    # 응답 확인 및 본문 추출\n",
    "    if response.status_code == 200:\n",
    "        text = response.text\n",
    "        df.at[index, \"본문\"] = text\n",
    "        #print(f\"기사 ID: {article_id}, URL: {article_url} 처리 성공\")\n",
    "    else:\n",
    "        print(f\"기사 ID: {article_id}, URL: {article_url}에 대한 요청 실패, 상태 코드: {response.status_code}\")\n",
    "        print(f\"응답: {response.text}\")\n",
    "\n",
    "    # 일정 시간 대기 (서버 부하 방지)\n",
    "    #time.sleep(1)\n",
    "\n",
    "# 엑셀 파일에 저장하면서 \"articleId\" 컬럼의 셀 서식을 텍스트로 지정\n",
    "output_file_path = os.path.join(base_path, f'{file_name} 기사.xlsx')\n",
    "with pd.ExcelWriter(output_file_path, engine='xlsxwriter') as writer:\n",
    "    df.to_excel(writer, index=False, sheet_name='Sheet1')\n",
    "    \n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Sheet1']\n",
    "    \n",
    "    # articleId 열을 텍스트 형식으로 설정\n",
    "    text_format = workbook.add_format({'num_format': '@'})\n",
    "    worksheet.set_column('G:G', None, text_format)\n",
    "\n",
    "print(f\"결과가 {output_file_path}에 저장되었습니다.\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
